from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "tiiuae/falcon-40b-instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("setup complete")

question = "what does the fox say?"

# Tokenize the input question
inputs = tokenizer.encode(question, return_tensors="pt")

# Generate a response from the model
output = model.generate(inputs, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)

# Decode the output tokens back into text
answer = tokenizer.decode(output[0], skip_special_tokens=True)

print(answer)